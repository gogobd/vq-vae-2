{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Not shown in this notebook:\n",
    "from vq_vae_2.examples.hierarchical.model import TopPrior, BottomPrior, make_vae\n",
    "from vq_vae_2.examples.hierarchical.data import load_images, load_tiled_images  # SwipeCropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'celebA256'\n",
    "LOAD_EPOCH = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = {\n",
    "    'celebA256': {\n",
    "        'VAE_PATH': './saved_states/hierarchical/celebA256/vae_{:08d}.pt',\n",
    "        'VAE_OPT_PATH': './saved_states/hierarchical/celebA256/vae_opt_{:08d}.pt',\n",
    "        'BOTTOM_PRIOR_PATH': './saved_states/hierarchical/celebA256/bottom_prior_{:08d}.pt',\n",
    "        #'BOTTOM_PRIOR_OPT_PATH': './saved_states/hierarchical/celebA256/bottom_prior_opt_{:08d}.pt',\n",
    "        'TOP_PRIOR_PATH': './saved_states/hierarchical/celebA256/top_prior_{:08d}.pt',\n",
    "        #'TOP_PRIOR_OPT_PATH': './saved_states/hierarchical/celebA256/top_prior_opt_{:08d}.pt',\n",
    "        'DATA_TRAIN': './data/celebA/data256x256/train',\n",
    "        'DATA_TEST': './data/celebA/data256x256/test',\n",
    "        'RESULTS': './results/hierarchical/celebA256/reconstructed_{:08d}.png',\n",
    "        'RESULTS_TEST': './results/hierarchical/celebA256/reconstructed_test_{:08d}.png',\n",
    "    },\n",
    "    'celebA256_tiled': {\n",
    "        'VAE_PATH': './saved_states/hierarchical/celebA256_tiled/vae_{:08d}.pt',\n",
    "        'VAE_OPT_PATH': './saved_states/hierarchical/celebA256_tiled/vae_opt_{:08d}.pt',\n",
    "        'BOTTOM_PRIOR_PATH': './saved_states/hierarchical/celebA256_tiled/bottom_prior_{:08d}.pt',\n",
    "        #'BOTTOM_PRIOR_OPT_PATH': './saved_states/hierarchical/celebA256_tiled/bottom_prior_opt_{:08d}.pt',\n",
    "        'TOP_PRIOR_PATH': './saved_states/hierarchical/celebA256_tiled/top_prior_{:08d}.pt',\n",
    "        #'TOP_PRIOR_OPT_PATH': './saved_states/hierarchical/celebA256_tiled/top_prior_opt_{:08d}.pt',\n",
    "        'DATA_TRAIN': './data/celebA/data1024x1024/train',\n",
    "        'DATA_TEST': './data/celebA/data1024x1024/test',\n",
    "        'RESULTS': './results/hierarchical/celebA_tiled/reconstructed_{:08d}.png',\n",
    "        'RESULTS_TEST': './results/hierarchical/celebA_tiled/reconstructed_test_{:08d}.png',\n",
    "    },\n",
    "    'DIV2K_tiled': {\n",
    "        'VAE_PATH': './saved_states/hierarchical/DIV2K_tiled/vae_{:08d}.pt',\n",
    "        'VAE_OPT_PATH': './saved_states/hierarchical/DIV2K_tiled/vae_opt_{:08d}.pt',\n",
    "        'BOTTOM_PRIOR_PATH': './saved_states/hierarchical/DIV2K_tiled/bottom_prior_{:08d}.pt',\n",
    "        #'BOTTOM_PRIOR_OPT_PATH': './saved_states/hierarchical/DIV2K_tiled/bottom_prior_opt_{:08d}.pt',\n",
    "        'TOP_PRIOR_PATH': './saved_states/hierarchical/DIV2K_tiled/top_prior_{:08d}.pt',\n",
    "        #'TOP_PRIOR_OPT_PATH': './saved_states/hierarchical/DIV2K_tiled/top_prior_opt_{:08d}.pt',\n",
    "        'DATA_TRAIN': './data/DIV2K/DIV2K_train_HR',\n",
    "        'DATA_TEST': './data/DIV2K/DIV2K_test_HR',\n",
    "        'RESULTS': './results/hierarchical/DIV2K_tiled/reconstructed_{:08d}.png',        \n",
    "        'RESULTS_TEST': './results/hierarchical/DIV2K_tiled/reconstructed_test_{:08d}.png',        \n",
    "    }\n",
    "}\n",
    "\n",
    "VAE_PATH = training_sets[NAME]['VAE_PATH']\n",
    "#VAE_OPT_PATH = training_sets[NAME]['VAE_OPT_PATH']\n",
    "BOTTOM_PRIOR_PATH = training_sets[NAME]['BOTTOM_PRIOR_PATH']\n",
    "#BOTTOM_PRIOR_OPT_PATH = training_sets[NAME]['BOTTOM_PRIOR_OPT_PATH']\n",
    "TOP_PRIOR_PATH = training_sets[NAME]['TOP_PRIOR_PATH']\n",
    "#TOP_PRIOR_OPT_PATH = training_sets[NAME]['TOP_PRIOR_OPT_PATH']\n",
    "DATA_TRAIN = training_sets[NAME]['DATA_TRAIN']\n",
    "RESULTS = training_sets[NAME]['RESULTS']\n",
    "RESULTS_TEST = training_sets[NAME]['RESULTS_TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(VAE_PATH.format(0)), exist_ok=True)\n",
    "#os.makedirs(os.path.dirname(VAE_OPT_PATH.format(0)), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(BOTTOM_PRIOR_PATH.format(0)), exist_ok=True)\n",
    "#os.makedirs(os.path.dirname(BOTTOM_PRIOR_OPT_PATH.format(0)), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(RESULTS.format(0)), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(RESULTS_TEST.format(0)), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a hierarchical VQ-VAE on 256x256 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(vae, images, RESULTS, i):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        recons = [torch.clamp(x, 0, 1).permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "                  for x in vae.full_reconstructions(images)]\n",
    "    vae.train()\n",
    "    top_recons, real_recons = recons\n",
    "    images = images.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "\n",
    "    columns = np.concatenate([top_recons, real_recons, images], axis=-2)\n",
    "    columns = np.concatenate(columns, axis=0)\n",
    "    Image.fromarray((columns * 255).astype('uint8')).save(\n",
    "        RESULTS.format(i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = make_vae()\n",
    "if os.path.exists(VAE_PATH.format(LOAD_EPOCH)):\n",
    "    model.load_state_dict(torch.load(VAE_PATH.format(LOAD_EPOCH), map_location='cpu'))\n",
    "model.to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "#if os.path.exists(VAE_OPT_PATH.format(LOAD_EPOCH)):\n",
    "#    model.load_state_dict(torch.load(VAE_OPT_PATH.format(LOAD_EPOCH), map_location='cpu'))\n",
    "# data = load_images(DATA_TRAIN)\n",
    "data = load_tiled_images(DATA_TRAIN, batch_size=12, width=256, height=256)\n",
    "for i in itertools.count(LOAD_EPOCH):\n",
    "    batch = next(data)\n",
    "    images = batch.to(DEVICE)\n",
    "    terms = model(images)\n",
    "    optimizer.zero_grad()\n",
    "    terms['loss'].backward()\n",
    "    optimizer.step()\n",
    "    model.revive_dead_entries()\n",
    "    if not i % 100:\n",
    "        print('step %d: mse=%f mse_top=%f' %\n",
    "             (i, terms['losses'][-1].item(), terms['losses'][0].item()))\n",
    "    if not i % 500:\n",
    "        torch.save(model.state_dict(), VAE_PATH.format(i))\n",
    "        #torch.save(optimizer.state_dict(), VAE_OPT_PATH.format(i))\n",
    "        save_reconstructions(model, images, RESULTS, i)\n",
    "#    with torch.no_grad():\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the bottom-level prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = make_vae()\n",
    "vae.load_state_dict(torch.load(VAE_PATH.format(LOAD_EPOCH), map_location='cpu'))\n",
    "vae.to(DEVICE)\n",
    "vae.eval()\n",
    "\n",
    "bottom_prior = BottomPrior()\n",
    "if os.path.exists(BOTTOM_PRIOR_PATH.format(LOAD_EPOCH)):\n",
    "    bottom_prior.load_state_dict(torch.load(BOTTOM_PRIOR_PATH.format(LOAD_EPOCH), map_location='cpu'))\n",
    "bottom_prior.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(bottom_prior.parameters(), lr=1e-4)\n",
    "#if os.path.exists(BOTTOM_PRIOR_OPT_PATH.format(LOAD_EPOCH)):\n",
    "#    optimizer.load_state_dict(torch.load(BOTTOM_PRIOR_OPT_PATH.format(LOAD_EPOCH), map_location=DEVICE))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "data = load_tiled_images(DATA_TRAIN, batch_size=4, width=256, height=256)\n",
    "for i in itertools.count(LOAD_EPOCH):\n",
    "    images = next(data).to(DEVICE)\n",
    "    bottom_enc = vae.encoders[0].encode(images)\n",
    "    _, _, bottom_idxs = vae.encoders[0].vq(bottom_enc)\n",
    "    _, _, top_idxs = vae.encoders[1](bottom_enc)\n",
    "    logits = bottom_prior(bottom_idxs, top_idxs)\n",
    "    logits = logits.permute(0, 2, 3, 1).contiguous()\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    loss = loss_fn(logits, bottom_idxs.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not i % 100:\n",
    "        print('step %d: loss=%f' % (i, loss.item()))\n",
    "    if not i % 500:\n",
    "        torch.save(bottom_prior.state_dict(), BOTTOM_PRIOR_PATH.format(i))\n",
    "        #torch.save(optimizer.state_dict(), BOTTOM_PRIOR_OPT_PATH.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the top-level prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = make_vae()\n",
    "vae.load_state_dict(torch.load(VAE_PATH.format(LOAD_EPOCH), map_location='cpu'))\n",
    "vae.to(DEVICE)\n",
    "vae.eval()\n",
    "\n",
    "top_prior = TopPrior()\n",
    "if os.path.exists(TOP_PRIOR_PATH.format(LOAD_EPOCH)):\n",
    "    top_prior.load_state_dict(torch.load(TOP_PRIOR_PATH.format(LOAD_EPOCH), map_location='cpu'))\n",
    "top_prior.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(top_prior.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "data = load_tiled_images(DATA_TRAIN, batch_size=4, width=256, height=256)\n",
    "for i in itertools.count(LOAD_EPOCH):\n",
    "    images = next(data).to(DEVICE)\n",
    "    _, _, encoded = vae.encoders[1](vae.encoders[0].encode(images))\n",
    "    logits = top_prior(encoded)\n",
    "    logits = logits.permute(0, 2, 3, 1).contiguous()\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    loss = loss_fn(logits, encoded.view(-1))\n",
    "    if not i % 100:\n",
    "        print('step %d: loss=%f' % (i, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not i % 500:\n",
    "        torch.save(top_prior.state_dict(), TOP_PRIOR_PATH.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples using the top-level prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 4\n",
    "\n",
    "def sample_softmax(probs):\n",
    "    number = random.random()\n",
    "    for i, x in enumerate(probs):\n",
    "        number -= x\n",
    "        if number <= 0:\n",
    "            return i\n",
    "    return len(probs) - 1\n",
    "\n",
    "vae = make_vae()\n",
    "vae.load_state_dict(torch.load(VAE_PATH.format(LOAD_EPOCH)))\n",
    "vae.to(DEVICE)\n",
    "vae.eval()\n",
    "\n",
    "top_prior = TopPrior()\n",
    "top_prior.load_state_dict(torch.load(TOP_PRIOR_PATH.format(LOAD_EPOCH)))\n",
    "top_prior.to(DEVICE)\n",
    "\n",
    "results = np.zeros([NUM_SAMPLES, 32, 32], dtype=np.long)\n",
    "for row in range(results.shape[1]):\n",
    "    for col in range(results.shape[2]):\n",
    "        partial_in = torch.from_numpy(results[:, :row + 1]).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = torch.softmax(top_prior(partial_in), dim=1).cpu().numpy()\n",
    "        for i, out in enumerate(outputs):\n",
    "            probs = out[:, row, col]\n",
    "            results[i, row, col] = sample_softmax(probs)\n",
    "    print('done row', row)\n",
    "with torch.no_grad():\n",
    "    full_latents = torch.from_numpy(results).to(DEVICE)\n",
    "    top_embedded = vae.encoders[1].vq.embed(full_latents)\n",
    "    bottom_encoded = vae.decoders[0]([top_embedded])\n",
    "    bottom_embedded, _, _ = vae.encoders[0].vq(bottom_encoded)\n",
    "    decoded = torch.clamp(vae.decoders[1]([top_embedded, bottom_embedded]), 0, 1)\n",
    "decoded = decoded.permute(0, 2, 3, 1).cpu().numpy()\n",
    "decoded = np.concatenate(decoded, axis=1)\n",
    "Image.fromarray((decoded * 255).astype(np.uint8)).save(os.path.join(RESULTS_TEST.format(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
