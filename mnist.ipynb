{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms\n",
    "\n",
    "from vq_vae_2.examples.mnist.model import Generator, make_vq_vae\n",
    "from vq_vae_2.examples.mnist.train_generator import load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./results/mnist', exist_ok=True)\n",
    "os.makedirs('./saved_states/mnist', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an encoder/decoder on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(batch, decoded):\n",
    "    batch = batch.detach().permute(0, 2, 3, 1).contiguous()\n",
    "    decoded = decoded.detach().permute(0, 2, 3, 1).contiguous()\n",
    "    input_images = (np.concatenate(batch.cpu().numpy(), axis=0) * 255).astype(np.uint8)\n",
    "    output_images = np.concatenate(decoded.cpu().numpy(), axis=0)\n",
    "    output_images = (np.clip(output_images, 0, 1) * 255).astype(np.uint8)\n",
    "    joined = np.concatenate([input_images[..., 0], output_images[..., 0]], axis=1)\n",
    "    Image.fromarray(joined).save('./results/mnist/reconstructions_{epoch:05d}.png'.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = make_vq_vae()\n",
    "if os.path.exists('./saved_states/nmist/vae.pt'):\n",
    "    vae.load_state_dict(torch.load('./saved_states/mnist/vae.pt', map_location=DEVICE))\n",
    "vae.to(DEVICE)\n",
    "optimizer = optim.Adam(vae.parameters())\n",
    "for i, batch in enumerate(load_images()):\n",
    "    batch = batch.to(DEVICE)\n",
    "    terms = vae(batch)\n",
    "    # import pdb;pdb.set_trace()\n",
    "    print(\n",
    "        'step {step}: loss={loss} losses={losses} reconstructions={reconstructions} embedded={embedded}'.format(\n",
    "            step=i,\n",
    "            loss=terms['loss'],\n",
    "            losses=terms['losses'],\n",
    "            reconstructions=terms['reconstructions'],\n",
    "            embedded=terms['embedded'],\n",
    "        )\n",
    "    )\n",
    "    optimizer.zero_grad()\n",
    "    terms['loss'].backward()\n",
    "    optimizer.step()\n",
    "    vae.revive_dead_entries()\n",
    "    if not i % 10:\n",
    "        torch.save(vae.state_dict(), './saved_states/mnist/vae.pt')\n",
    "    if not i % 100:\n",
    "        save_reconstructions(batch, terms['reconstructions'][-1], epoch=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PixelCNN on MNIST using a pre-trained VQ-VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(train=True):\n",
    "    while True:\n",
    "        for data, _ in create_data_loader(train):\n",
    "            yield data\n",
    "\n",
    "\n",
    "def create_data_loader(train):\n",
    "    mnist = torchvision.datasets.MNIST('./data/mnist', train=train, download=True,\n",
    "                                       transform=torchvision.transforms.ToTensor())\n",
    "    return torch.utils.data.DataLoader(mnist, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = make_vq_vae()\n",
    "vae.load_state_dict(torch.load('./saved_states/mnist/vae.pt', map_location=DEVICE))\n",
    "vae.to(DEVICE)\n",
    "vae.eval()\n",
    "\n",
    "generator = Generator()\n",
    "if os.path.exists('./saved_states/minst/gen.pt'):\n",
    "    generator.load_state_dict(torch.load('./saved_states/mnist/gen.pt', map_location=DEVICE))\n",
    "generator.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(generator.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "test_images = load_images(train=False)\n",
    "for batch_idx, images in enumerate(load_images()):\n",
    "    images = images.to(DEVICE)\n",
    "    losses = []\n",
    "    for img_set in [images, next(test_images).to(DEVICE)]:\n",
    "        _, _, encoded = vae.encoders[0](img_set)\n",
    "        logits = generator(encoded)\n",
    "        logits = logits.permute(0, 2, 3, 1).contiguous()\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        losses.append(loss_fn(logits, encoded.view(-1)))\n",
    "    optimizer.zero_grad()\n",
    "    losses[0].backward()\n",
    "    optimizer.step()\n",
    "    print('train=%f test=%f' % (losses[0].item(), losses[1].item()))\n",
    "    if not batch_idx % 100:\n",
    "        torch.save(generator.state_dict(), './saved_states/mnist/gen.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample an image from a PixelCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_softmax(probs):\n",
    "    number = random.random()\n",
    "    for i, x in enumerate(probs):\n",
    "        number -= x\n",
    "        if number <= 0:\n",
    "            return i\n",
    "    return len(probs) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = make_vq_vae()\n",
    "vae.load_state_dict(torch.load('./saved_states/mnist/vae.pt', map_location=DEVICE))\n",
    "vae.to(DEVICE)\n",
    "vae.eval()\n",
    "generator = Generator()\n",
    "generator.load_state_dict(torch.load('./saved_states/mnist/gen.pt', map_location=DEVICE))\n",
    "generator.to(DEVICE)\n",
    "\n",
    "inputs = np.zeros([4, 7, 7], dtype=np.long)\n",
    "for row in range(7):\n",
    "    for col in range(7):\n",
    "        with torch.no_grad():\n",
    "            outputs = torch.softmax(generator(torch.from_numpy(inputs).to(DEVICE)), dim=1)\n",
    "            for i, out in enumerate(outputs.cpu().numpy()):\n",
    "                probs = out[:, row, col]\n",
    "                inputs[i, row, col] = sample_softmax(probs)\n",
    "    print('done row', row)\n",
    "embedded = vae.encoders[0].vq.embed(torch.from_numpy(inputs).to(DEVICE))\n",
    "decoded = torch.clamp(vae.decoders[0]([embedded]), 0, 1).detach().cpu().numpy()\n",
    "decoded = np.concatenate(decoded, axis=1)\n",
    "Image.fromarray((decoded * 255).astype(np.uint8)[0]).save('./results/mnist/samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
